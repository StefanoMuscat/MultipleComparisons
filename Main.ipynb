{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "from math import log\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_counters(data_sets, c_threshold=None, w_factor=\"fraction\"):\n",
    "    \"\"\"Calculate 1-similarity, 0-similarity, and dissimilarity counters\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data_sets : np.ndarray\n",
    "        Array of arrays. Each sub-array contains m + 1 elements,\n",
    "        with m being the length of the fingerprints. The first\n",
    "        m elements are the column sums of the matrix of fingerprints.\n",
    "        The last element is the number of fingerprints.\n",
    "\n",
    "    c_threshold : {None, 'dissimilar', int}\n",
    "        Coincidence threshold.\n",
    "        None : Default, c_threshold = n_fingerprints % 2\n",
    "        'dissimilar' : c_threshold = ceil(n_fingerprints / 2)\n",
    "        int : Integer number < n_fingerprints\n",
    "\n",
    "    w_factor : {\"fraction\", \"power_n\"}\n",
    "        Type of weight function that will be used.\n",
    "        'fraction' : similarity = d[k]/n\n",
    "                     dissimilarity = 1 - (d[k] - n_fingerprints % 2)/n_fingerprints\n",
    "        'power_n' : similarity = n**-(n_fingerprints - d[k])\n",
    "                    dissimilarity = n**-(d[k] - n_fingerprints % 2)\n",
    "        other values : similarity = dissimilarity = 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counters : dict\n",
    "        Dictionary with the weighted and non-weighted counters.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Please, cite the original papers on the n-ary indices:\n",
    "    https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00505-3\n",
    "    https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00504-4\n",
    "    \"\"\"\n",
    "    # Setting matches\n",
    "    total_data = np.sum(data_sets, axis=0)\n",
    "    n_fingerprints = int(total_data[-1])\n",
    "    c_total = total_data[:-1]\n",
    "    \n",
    "    # Assign c_threshold\n",
    "    if not c_threshold:\n",
    "        c_threshold = n_fingerprints % 2\n",
    "    if isinstance(c_threshold, str):\n",
    "        if c_threshold != 'dissimilar':\n",
    "            raise TypeError(\"c_threshold must be None, 'dissimilar', or an integer.\")\n",
    "        else:\n",
    "            c_threshold = ceil(n_fingerprints / 2)\n",
    "    if isinstance(c_threshold, int):\n",
    "        if c_threshold >= n_fingerprints:\n",
    "            raise ValueError(\"c_threshold cannot be equal or greater than n_fingerprints.\")\n",
    "        c_threshold = c_threshold\n",
    "    \n",
    "    # Set w_factor\n",
    "    if w_factor:\n",
    "        if \"power\" in w_factor:\n",
    "            power = int(w_factor.split(\"_\")[-1])\n",
    "            def f_s(d):\n",
    "                return power**-float(n_fingerprints - d)\n",
    "    \n",
    "            def f_d(d):\n",
    "                return power**-float(d - n_fingerprints % 2)\n",
    "        elif w_factor == \"fraction\":\n",
    "            def f_s(d):\n",
    "                return d/n_fingerprints\n",
    "    \n",
    "            def f_d(d):\n",
    "                return 1 - (d - n_fingerprints % 2)/n_fingerprints\n",
    "        else:\n",
    "            def f_s(d):\n",
    "                return 1\n",
    "    \n",
    "            def f_d(d):\n",
    "                return 1\n",
    "    else:\n",
    "        def f_s(d):\n",
    "            return 1\n",
    "    \n",
    "        def f_d(d):\n",
    "            return 1\n",
    "    \n",
    "    # Calculate a, d, b + c\n",
    "    a = 0\n",
    "    w_a = 0\n",
    "    d = 0\n",
    "    w_d = 0\n",
    "    total_dis = 0\n",
    "    total_w_dis = 0\n",
    "    for s in c_total:\n",
    "        if 2 * s - n_fingerprints > c_threshold:\n",
    "            a += 1\n",
    "            w_a += f_s(2 * s - n_fingerprints)\n",
    "        elif n_fingerprints - 2 * s > c_threshold:\n",
    "            d += 1\n",
    "            w_d += f_s(abs(2 * s - n_fingerprints))\n",
    "        else:\n",
    "            total_dis += 1\n",
    "            total_w_dis += f_d(abs(2 * s - n_fingerprints))\n",
    "    total_sim = a + d\n",
    "    total_w_sim = w_a + w_d\n",
    "    p = total_sim + total_dis\n",
    "    w_p = total_w_sim + total_w_dis\n",
    "    \n",
    "    counters = {\"a\": a, \"w_a\": w_a, \"d\": d, \"w_d\": w_d,\n",
    "                \"total_sim\": total_sim, \"total_w_sim\": total_w_sim,\n",
    "                \"total_dis\": total_dis, \"total_w_dis\": total_w_dis,\n",
    "                \"p\": p, \"w_p\": w_p}\n",
    "    \n",
    "    return counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_medoid(total_data, n_ary = 'RR', weight = 'nw'):\n",
    "    \"\"\"Calculate the medoid of a set\"\"\"\n",
    "    index = len(total_data[0]) + 1\n",
    "    min_sim = 3.08\n",
    "    total_sum = np.sum(total_data, axis = 0)\n",
    "    for i, pixel in enumerate(total_data):\n",
    "        i_sum = total_sum - total_data[i]\n",
    "        data_sets = [np.append(i_sum, len(total_data) - 1)]\n",
    "        Indices = gen_sim_dict(data_sets)\n",
    "        sim_index = Indices[weight][n_ary]\n",
    "        if sim_index < min_sim:\n",
    "            min_sim = sim_index\n",
    "            index = i\n",
    "        else:\n",
    "            pass\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_dict(data_sets, c_threshold=None, w_factor=\"fraction\"):\n",
    "    counters = calculate_counters(data_sets, c_threshold=c_threshold, w_factor=\"fraction\")\n",
    "    # Indices\n",
    "    # AC: Austin-Colwell, BUB: Baroni-Urbani-Buser, CTn: Consoni-Todschini n\n",
    "    # Fai: Faith, Gle: Gleason, Ja: Jaccard, Ja0: Jaccard 0-variant\n",
    "    # JT: Jaccard-Tanimoto, RT: Rogers-Tanimoto, RR: Russel-Rao\n",
    "    # SM: Sokal-Michener, SSn: Sokal-Sneath n\n",
    "\n",
    "    # Weighted Indices\n",
    "    ac_w = (2/np.pi) * np.arcsin(np.sqrt(counters['total_w_sim']/\n",
    "                                         counters['w_p']))\n",
    "    bub_w = ((counters['w_a'] * counters['w_d'])**0.5 + counters['w_a'])/\\\n",
    "            ((counters['w_a'] * counters['w_d'])**0.5 + counters['w_a'] + counters['total_w_dis'])\n",
    "    ct1_w = (log(1 + counters['w_a'] + counters['w_d']))/\\\n",
    "            (log(1 + counters['w_p']))\n",
    "    ct2_w = (log(1 + counters['w_p']) - log(1 + counters['total_w_dis']))/\\\n",
    "            (log(1 + counters['w_p']))\n",
    "    ct3_w = (log(1 + counters['w_a']))/\\\n",
    "            (log(1 + counters['w_p']))\n",
    "    ct4_w = (log(1 + counters['w_a']))/\\\n",
    "            (log(1 + counters['w_a'] + counters['total_w_dis']))\n",
    "    fai_w = (counters['w_a'] + 0.5 * counters['w_d'])/\\\n",
    "            (counters['w_p'])\n",
    "    gle_w = (2 * counters['w_a'])/\\\n",
    "            (2 * counters['w_a'] + counters['total_w_dis'])\n",
    "    ja_w = (3 * counters['w_a'])/\\\n",
    "           (3 * counters['w_a'] + counters['total_w_dis'])\n",
    "    ja0_w = (3 * counters['total_w_sim'])/\\\n",
    "            (3 * counters['total_w_sim'] + counters['total_w_dis'])\n",
    "    jt_w = (counters['w_a'])/\\\n",
    "           (counters['w_a'] + counters['total_w_dis'])\n",
    "    rt_w = (counters['total_w_sim'])/\\\n",
    "           (counters['w_p'] + counters['total_w_dis'])\n",
    "    rr_w = (counters['w_a'])/\\\n",
    "           (counters['w_p'])\n",
    "    sm_w =(counters['total_w_sim'])/\\\n",
    "          (counters['w_p'])\n",
    "    ss1_w = (counters['w_a'])/\\\n",
    "            (counters['w_a'] + 2 * counters['total_w_dis'])\n",
    "    ss2_w = (2 * counters['total_w_sim'])/\\\n",
    "            (counters['w_p'] + counters['total_w_sim'])\n",
    "\n",
    "\n",
    "    ## Non-Weighted Indices\n",
    "    ac_nw = (2/np.pi) * np.arcsin(np.sqrt(counters['total_w_sim']/\n",
    "                                          counters['p']))\n",
    "    bub_nw = ((counters['w_a'] * counters['w_d'])**0.5 + counters['w_a'])/\\\n",
    "             ((counters['a'] * counters['d'])**0.5 + counters['a'] + counters['total_dis'])\n",
    "    ct1_nw = (log(1 + counters['w_a'] + counters['w_d']))/\\\n",
    "             (log(1 + counters['p']))\n",
    "    ct2_nw = (log(1 + counters['w_p']) - log(1 + counters['total_w_dis']))/\\\n",
    "             (log(1 + counters['p']))\n",
    "    ct3_nw = (log(1 + counters['w_a']))/\\\n",
    "             (log(1 + counters['p']))\n",
    "    ct4_nw = (log(1 + counters['w_a']))/\\\n",
    "             (log(1 + counters['a'] + counters['total_dis']))\n",
    "    fai_nw = (counters['w_a'] + 0.5 * counters['w_d'])/\\\n",
    "             (counters['p'])\n",
    "    gle_nw = (2 * counters['w_a'])/\\\n",
    "             (2 * counters['a'] + counters['total_dis'])\n",
    "    ja_nw = (3 * counters['w_a'])/\\\n",
    "            (3 * counters['a'] + counters['total_dis'])\n",
    "    ja0_nw = (3 * counters['total_w_sim'])/\\\n",
    "             (3 * counters['total_sim'] + counters['total_dis'])\n",
    "    jt_nw = (counters['w_a'])/\\\n",
    "            (counters['a'] + counters['total_dis'])\n",
    "    rt_nw = (counters['total_w_sim'])/\\\n",
    "            (counters['p'] + counters['total_dis'])\n",
    "    rr_nw = (counters['w_a'])/\\\n",
    "            (counters['p'])\n",
    "    sm_nw =(counters['total_w_sim'])/\\\n",
    "           (counters['p'])\n",
    "    ss1_nw = (counters['w_a'])/\\\n",
    "             (counters['a'] + 2 * counters['total_dis'])\n",
    "    ss2_nw = (2 * counters['total_w_sim'])/\\\n",
    "             (counters['p'] + counters['total_sim'])\n",
    "\n",
    "    # Dictionary with all the results\n",
    "    Indices = {'nw': {'AC':ac_nw,\n",
    "                      'BUB':bub_nw,\n",
    "                      'CT1':ct1_nw,\n",
    "                      'CT2':ct2_nw,\n",
    "                      'CT3':ct3_nw,\n",
    "                      'CT4':ct4_nw,\n",
    "                      'Fai':fai_nw,\n",
    "                      'Gle':gle_nw,\n",
    "                      'Ja0':ja0_nw,\n",
    "                      'Ja':ja_nw,\n",
    "                      'JT':jt_nw,\n",
    "                      'RT':rt_nw,\n",
    "                      'RR':rr_nw,\n",
    "                      'SM':sm_nw,\n",
    "                      'SS1':ss1_nw,\n",
    "                      'SS2':ss2_nw},\n",
    "                'w': {'AC':ac_w,\n",
    "                      'BUB':bub_w,\n",
    "                      'CT1':ct1_w,\n",
    "                      'CT2':ct2_w,\n",
    "                      'CT3':ct3_w,\n",
    "                      'CT4':ct4_w,\n",
    "                      'Fai':fai_w,\n",
    "                      'Gle':gle_w,\n",
    "                      'Ja0':ja0_w,\n",
    "                      'Ja':ja_w,\n",
    "                      'JT':jt_w,\n",
    "                      'RT':rt_w,\n",
    "                      'RR':rr_w,\n",
    "                      'SM':sm_w,\n",
    "                      'SS1':ss1_w,\n",
    "                      'SS2':ss2_w}}\n",
    "    return Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier(total_data, n_ary = 'RR', weight = 'nw'):\n",
    "    \"\"\"Calculate the outlier of a set\"\"\"\n",
    "    index = len(total_data[0]) + 1\n",
    "    max_sim = -3.08\n",
    "    total_sum = np.sum(total_data, axis = 0)\n",
    "    for i, pixel in enumerate(total_data):\n",
    "        i_sum = total_sum - total_data[i]\n",
    "        data_sets = [np.append(i_sum, len(total_data) - 1)]\n",
    "        Indices = gen_sim_dict(data_sets)\n",
    "        sim_index = Indices[weight][n_ary]\n",
    "        if sim_index > max_sim:\n",
    "            max_sim = sim_index\n",
    "            index = i\n",
    "        else:\n",
    "            pass\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_index(total_data, indices, selected_n, c_threshold=None, n_ary = 'RR', weight = 'nw'):\n",
    "    \"\"\"Binary tie-breaker selection criterion\"\"\"\n",
    "    index = len(total_data[0]) + 1\n",
    "    min_value = 3.08\n",
    "    for i in indices:\n",
    "        v = 0\n",
    "        for j in selected_n:\n",
    "            c_total = total_data[j] + total_data[i]\n",
    "            data_sets = [np.append(c_total, 2)]\n",
    "            Indices = gen_sim_dict(data_sets, c_threshold=c_threshold)\n",
    "            sim_index = Indices[weight][n_ary]\n",
    "            v += sim_index\n",
    "        av_v = v/(len(selected_n) + 1)\n",
    "        if av_v < min_value:\n",
    "            index = i\n",
    "            min_value = av_v\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_index_n(total_data, selected_condensed, n, select_from_n, selected_n, c_threshold=None,\n",
    "                    n_ary = 'RR', weight = 'nw'):\n",
    "    \"\"\"Select a diverse object using the ECS_MeDiv algorithm\"\"\"\n",
    "    n_total = n + 1\n",
    "    # min value that is guaranteed to be higher than all the comparisons\n",
    "    min_value = 3.08\n",
    "    \n",
    "    # placeholder index\n",
    "    indices = [len(total_data[0]) + 1]\n",
    "    \n",
    "    # for all indices that have not been selected\n",
    "    for i in select_from_n:\n",
    "        # column sum\n",
    "        c_total = selected_condensed + total_data[i]\n",
    "        # calculating similarity\n",
    "        data_sets = [np.append(c_total, n_total)]\n",
    "        Indices = gen_sim_dict(data_sets, c_threshold=c_threshold)\n",
    "        sim_index = Indices[weight][n_ary]\n",
    "        # if the sim of the set is less than the similarity of the previous diverse set, update min_value and index\n",
    "        if sim_index < min_value:\n",
    "            indices = [i]\n",
    "            min_value = sim_index\n",
    "        elif sim_index == min_value:\n",
    "            indices.append(i)\n",
    "    if len(indices) == 1:\n",
    "        index = indices[0]\n",
    "    else:\n",
    "        # Use average of binary similarities as tie-breaker\n",
    "        index = get_single_index(total_data, indices, selected_n, c_threshold=None, n_ary = n_ary, weight = 'nw')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fingerprints:  10\n",
      "Condensed fingerprints:  [5 5 5 4]\n",
      "Data sets:  [[ 5  5  5  4 10]]\n"
     ]
    }
   ],
   "source": [
    "# Sample calculation\n",
    "\n",
    "# Fingerprints\n",
    "fingerprints = np.array([[1, 0, 1, 0],\n",
    "                         [0, 0, 1, 1],\n",
    "                         [1, 1, 1, 0],\n",
    "                         [1, 0, 0, 0],\n",
    "                         [0, 1, 1, 0],\n",
    "                         [0, 1, 0, 1],\n",
    "                         [1, 1, 0, 0],\n",
    "                         [1, 0, 1, 1],\n",
    "                         [0, 1, 0, 0],\n",
    "                         [0, 0, 0, 1]])\n",
    "\n",
    "# Number of fingerprints\n",
    "n_fingerprints = len(fingerprints)\n",
    "\n",
    "# Column sums\n",
    "condensed = np.sum(fingerprints, axis = 0)\n",
    "\n",
    "# Generate datasets\n",
    "data_sets = np.array([np.append(condensed, n_fingerprints)])\n",
    "\n",
    "#Print the number of fingerprints and condensed fingerprints\n",
    "print(\"Number of fingerprints: \", n_fingerprints)\n",
    "print(\"Condensed fingerprints: \", condensed)\n",
    "print(\"Data sets: \", data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counters\n",
    "counters = calculate_counters(data_sets, c_threshold=0, w_factor=\"fraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'w_a': 0,\n",
       " 'd': 1,\n",
       " 'w_d': 0.2,\n",
       " 'total_sim': 1,\n",
       " 'total_w_sim': 0.2,\n",
       " 'total_dis': 3,\n",
       " 'total_w_dis': 3.0,\n",
       " 'p': 4,\n",
       " 'w_p': 3.2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nw': {'AC': 0.14356629312870628, 'BUB': 0.0, 'CT1': 0.11328275255937834, 'CT2': 0.03031503346136685, 'CT3': 0.0, 'CT4': 0.0, 'Fai': 0.025, 'Gle': 0.0, 'Ja0': 0.10000000000000002, 'Ja': 0.0, 'JT': 0.0, 'RT': 0.028571428571428574, 'RR': 0.0, 'SM': 0.05, 'SS1': 0.0, 'SS2': 0.08}, 'w': {'AC': 0.16086124651033248, 'BUB': 0.0, 'CT1': 0.1270458663451878, 'CT2': 0.033998111825222097, 'CT3': 0.0, 'CT4': 0.0, 'Fai': 0.03125, 'Gle': 0.0, 'Ja0': 0.16666666666666669, 'Ja': 0.0, 'JT': 0.0, 'RT': 0.03225806451612903, 'RR': 0.0, 'SM': 0.0625, 'SS1': 0.0, 'SS2': 0.11764705882352941}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate indices\n",
    "indices = gen_sim_dict(data_sets, c_threshold=0, w_factor=\"fraction\")\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arys = ['AC', 'BUB', 'CT1', 'CT2', 'CT3', 'CT4', 'Fai',\n",
    "          'Gle', 'Ja', 'Ja0', 'JT', 'RT', 'RR', 'SM', 'SS1', 'SS2']\n",
    "\n",
    "for n_ary in n_arys:\n",
    "    DiversityDict = {}\n",
    "    for c_threshold in ['min']:\n",
    "        for file in glob.glob('*.npy'):\n",
    "            if 'rank' in file:\n",
    "                pass\n",
    "            else:\n",
    "                base_name = file.split('.')[0]\n",
    "                # Seed selection:\n",
    "                # medoid = start from medoid\n",
    "                # random = select random initial seed\n",
    "                # out = start from outlier\n",
    "                start = 'medoid'\n",
    "                # Numpy array with the data\n",
    "                total_data = np.load(file)\n",
    "                total_n = []\n",
    "                \n",
    "                # total number of fingerprints\n",
    "                fp_total = len(total_data)\n",
    "                \n",
    "                # indices of all the fingerprints\n",
    "                total_indices = np.array(range(fp_total))\n",
    "                \n",
    "                # starting point\n",
    "                if start =='medoid':\n",
    "                    seed = calculate_medoid(total_data, n_ary = n_ary)\n",
    "                elif start == 'random':\n",
    "                    seed = random.randint(0, fp_total - 1)\n",
    "                elif start == 'out':\n",
    "                    seed = calculate_outlier(total_data, n_ary = n_ary)\n",
    "                else:\n",
    "                    print('Select a correct starting point')\n",
    "                selected_n = [seed]\n",
    "                \n",
    "                # vector with the column sums of all the selected fingerprints\n",
    "                selected_condensed = total_data[seed]\n",
    "                \n",
    "                # number of fingerprints selected\n",
    "                n = 1\n",
    "                while len(selected_n) < 10:\n",
    "                    # indices from which to select the new fingerprints\n",
    "                    select_from_n = np.delete(total_indices, selected_n)\n",
    "                    \n",
    "                    # new index selected\n",
    "                    new_index_n = get_new_index_n(total_data, selected_condensed, n, select_from_n, selected_n,\n",
    "                                                  c_threshold=c_threshold, n_ary = n_ary)\n",
    "                    \n",
    "                    # updating column sum vector\n",
    "                    selected_condensed += total_data[new_index_n]\n",
    "                    \n",
    "                    # updating selected indices\n",
    "                    selected_n.append(new_index_n)\n",
    "                    \n",
    "                    # updating n\n",
    "                    n = len(selected_n)\n",
    "                    print(selected_n)\n",
    "                DiversityDict[c_threshold][base_name] = selected_n\n",
    "    # f = open(n_ary + '_' + start + '_' + base_name + '_.pkl', 'wb')\n",
    "    # pickle.dump(DiversityDict, f)\n",
    "    # f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DiversityDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
